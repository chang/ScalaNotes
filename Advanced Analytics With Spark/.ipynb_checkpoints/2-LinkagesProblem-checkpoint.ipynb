{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linkages Problem\n",
    "\n",
    "From Advanced Analytics with Spark, 2nd Edition.  \n",
    "*2. Introduction to Data Analysis with Scala and Spark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dat = [id_1: int, id_2: int ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "class org.apache.spark.sql.Dataset"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read in the data. Since we're on running on local Spark, only read 1/10 partitions (~500k rows)\n",
    "val dat = spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"nullValue\", \"?\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"./data/linkages.csv\")\n",
    "dat.getClass\n",
    "\n",
    "/*\n",
    "Using schema inference requires an extra pass over the data. \n",
    "If we know the schema ahead of time, define it using `org.apache.spark.sql.types.StructType`,\n",
    "and pass it to the reader. \n",
    "This can improve performance significantly.\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574913"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Exploratory analysis\n",
    "dat.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_1: integer (nullable = true)\n",
      " |-- id_2: integer (nullable = true)\n",
      " |-- cmp_fname_c1: double (nullable = true)\n",
      " |-- cmp_fname_c2: double (nullable = true)\n",
      " |-- cmp_lname_c1: double (nullable = true)\n",
      " |-- cmp_lname_c2: double (nullable = true)\n",
      " |-- cmp_sex: integer (nullable = true)\n",
      " |-- cmp_bd: integer (nullable = true)\n",
      " |-- cmp_bm: integer (nullable = true)\n",
      " |-- cmp_by: integer (nullable = true)\n",
      " |-- cmp_plz: integer (nullable = true)\n",
      " |-- is_match: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dat.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><td>37291</td><td>53113</td><td>0.833333333333333</td><td>NULL</td><td>1.0</td><td>NULL</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>true</td></tr>\n",
       "<tr><td>39086</td><td>47614</td><td>1.0</td><td>NULL</td><td>1.0</td><td>NULL</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>true</td></tr>\n",
       "<tr><td>70031</td><td>70237</td><td>1.0</td><td>NULL</td><td>1.0</td><td>NULL</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>true</td></tr>\n",
       "<tr><td>84795</td><td>97439</td><td>1.0</td><td>NULL</td><td>1.0</td><td>NULL</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>true</td></tr>\n",
       "<tr><td>36950</td><td>42116</td><td>1.0</td><td>NULL</td><td>1.0</td><td>1.0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>true</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-------+-------+-------------------+------+-----+------+-----+-----+-----+-----+-----+------+\n",
       "| 37291 | 53113 | 0.833333333333333 | NULL | 1.0 | NULL | 1   | 1   | 1   | 1   | 0   | true |\n",
       "| 39086 | 47614 | 1.0               | NULL | 1.0 | NULL | 1   | 1   | 1   | 1   | 1   | true |\n",
       "| 70031 | 70237 | 1.0               | NULL | 1.0 | NULL | 1   | 1   | 1   | 1   | 1   | true |\n",
       "| 84795 | 97439 | 1.0               | NULL | 1.0 | NULL | 1   | 1   | 1   | 1   | 1   | true |\n",
       "| 36950 | 42116 | 1.0               | NULL | 1.0 | 1.0  | 1   | 1   | 1   | 1   | 1   | true |\n",
       "+-------+-------+-------------------+------+-----+------+-----+-----+-----+-----+-----+------+"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the above 3 operations, Spark has reread and reprocessed the data each time. To avoid this, we `.cache()` the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><td>37291</td><td>53113</td><td>0.833333333333333</td><td>NULL</td><td>1.0</td><td>NULL</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>true</td></tr>\n",
       "<tr><td>39086</td><td>47614</td><td>1.0</td><td>NULL</td><td>1.0</td><td>NULL</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>true</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-------+-------+-------------------+------+-----+------+-----+-----+-----+-----+-----+------+\n",
       "| 37291 | 53113 | 0.833333333333333 | NULL | 1.0 | NULL | 1   | 1   | 1   | 1   | 0   | true |\n",
       "| 39086 | 47614 | 1.0               | NULL | 1.0 | NULL | 1   | 1   | 1   | 1   | 1   | true |\n",
       "+-------+-------+-------------------+------+-----+------+-----+-----+-----+-----+-----+------+"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.cache()\n",
    "dat.count\n",
    "dat.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on Caching\n",
    "\n",
    "> Spark defines a few different mechanisms, or StorageLevel values, for persisting data. cache() is shorthand for persist(StorageLevel.MEMORY), which stores the rows as unserialized Java objects. When Spark estimates that a partition will not fit in memory, it simply will not store it, and it will be recomputed the next time it’s needed. This level makes the most sense when the objects will be referenced frequently and/or require low-latency access, because it avoids any serialization overhead. Its drawback is that it takes up larger amounts of memory than its alternatives. Also, holding on to many small objects puts pressure on Java’s garbage collection, which can result in stalls and general slowness.\n",
    "\n",
    "> Spark also exposes a MEMORY_SER storage level, which allocates large byte buffers in memory and serializes the records into them. When we use the right format (more on this in a bit), serialized data usually takes up two to five times less space than its raw equivalent.\n",
    "\n",
    "> Spark can use disk for caching data as well. The MEMORY_AND_DISK and MEMORY_AND_DISK_SER are similar to the MEMORY and MEMORY_SER storage levels, respectively. For the latter two, if a partition will not fit in memory, it is simply not stored, meaning that it must be recomputed from its dependencies the next time an action uses it. For the former, Spark spills partitions that will not fit in memory to disk.\n",
    "\n",
    "> Although both DataFrames and RDDs can be cached, Spark can use the detailed knowledge of the data stored with a data frame available via the DataFrame’s schema to persist the data far more efficiently than it can with Java objects stored inside of RDDs.\n",
    "\n",
    "> Deciding when to cache data can be an art. The decision typically involves trade-offs between space and speed, with the specter of garbage-collecting looming overhead to occasionally confound things further. In general, data should be cached when it is likely to be referenced by multiple actions, is relatively small compared to the amount of memory/disk available on the cluster, and is expensive to regenerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 63:>                                                         (0 + 0) / 7]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datRdd = MapPartitionsRDD[155] at rdd at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(false -> 572820, true -> 2093)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// How many records are matches?\n",
    "\n",
    "// Using the RDD API\n",
    "val datRdd = dat.rdd\n",
    "datRdd.map((x: org.apache.spark.sql.Row) => x.getAs[Boolean](\"is_match\")).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+                                                               \n",
      "|is_match| count|\n",
      "+--------+------+\n",
      "|   false|572820|\n",
      "|    true|  2093|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Using the DataFrame API\n",
    "dat.groupBy(\"is_match\")\n",
    "    .count()\n",
    "    .orderBy($\"count\".desc)\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column references\n",
    "There are two ways to reference columns:  \n",
    "\n",
    "`.groupBy(\"is_match\")` - string literal  \n",
    "`.groupBy($\"is_match\")` - column object\n",
    "\n",
    "In the groupBy statement, both would work, but we need to use the Column object notation for orderBy since `.desc` is a method that works on `Column` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------------+-----------------+-----------------+\n",
      "| avg(cmp_fname_c1)|stddev_samp(cmp_fname_c1)|max(cmp_fname_c1)|min(cmp_fname_c1)|\n",
      "+------------------+-------------------------+-----------------+-----------------+\n",
      "|0.7127592938253411|       0.3889286452463531|              1.0|              0.0|\n",
      "+------------------+-------------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// multiple aggregation functions\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "dat.agg(avg($\"cmp_fname_c1\"),\n",
    "        stddev($\"cmp_fname_c1\"),\n",
    "        max($\"cmp_fname_c1\"),\n",
    "        min($\"cmp_fname_c1\"))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|39086|47614|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|70031|70237|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|84795|97439|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|36950|42116|         1.0|        null|         1.0|         1.0|      1|     1|     1|     1|      1|    true|\n",
      "|42413|48491|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// saving the dataframe as a View to access with SQL\n",
    "// this dataframe is TEMPORARY and is only available in this session\n",
    "dat.createOrReplaceTempView(\"dataset\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM dataset\n",
    "    WHERE is_match = true AND cmp_plz = 1\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+              \n",
      "|summary|              id_1|      cmp_fname_c1|      cmp_fname_c2|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|            574913|            574811|             10325|\n",
      "|   mean|33271.962171667714|0.7127592938253411|0.8977586763518969|\n",
      "| stddev|23622.669425933756|0.3889286452463531|0.2742577520430532|\n",
      "|    min|                 1|               0.0|               0.0|\n",
      "|    max|             99894|               1.0|               1.0|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summary = [summary: string, id_1: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[summary: string, id_1: string ... 10 more fields]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Summary statistics\n",
    "val summary = dat.describe()\n",
    "summary.select(\"summary\", \"id_1\", \"cmp_fname_c1\", \"cmp_fname_c2\").show()\n",
    "\n",
    "// note that cmp_fname_c2 has many more nulls than cmp_fname_c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 158:================================================>        (6 + 1) / 7]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matches = [id_1: int, id_2: int ... 10 more fields]\n",
       "matchesSummary = [summary: string, id_1: string ... 10 more fields]\n",
       "misses = [id_1: int, id_2: int ... 10 more fields]\n",
       "missesSummary = [summary: string, id_1: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[summary: string, id_1: string ... 10 more fields]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Summary statistics with the subset where is_match == true\n",
    "\n",
    "val matches = dat.filter($\"is_match\" === true)  // using DataFrame API\n",
    "val matchesSummary = matches.describe()\n",
    "\n",
    "val misses = dat.where(\"is_match = false\")  // using Spark SQL\n",
    "val missesSummary = misses.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|summary|             cmp_plz|      cmp_fname_c1|\n",
      "+-------+--------------------+------------------+\n",
      "|  count|              573618|            574811|\n",
      "|   mean|0.005494946113964...|0.7127592938253411|\n",
      "| stddev| 0.07392402321301972|0.3889286452463531|\n",
      "|    min|                   0|               0.0|\n",
      "|    max|                   1|               1.0|\n",
      "+-------+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Reshaping the previously calculated summary DataFrame from long format...\n",
    "summary.select(\"summary\", \"cmp_plz\", \"cmp_fname_c1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+\n",
      "|    _1|     _2|                  _3|\n",
      "+------+-------+--------------------+\n",
      "| count|cmp_plz|            573618.0|\n",
      "|  mean|cmp_plz|0.005494946113964...|\n",
      "|stddev|cmp_plz| 0.07392402321301972|\n",
      "|   min|cmp_plz|                 0.0|\n",
      "|   max|cmp_plz|                 1.0|\n",
      "+------+-------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "schema = StructType(StructField(summary,StringType,true), StructField(id_1,StringType,true), StructField(id_2,StringType,true), StructField(cmp_fname_c1,StringType,true), StructField(cmp_fname_c2,StringType,true), StructField(cmp_lname_c1,StringType,true), StructField(cmp_lname_c2,StringType,true), StructField(cmp_sex,StringType,true), StructField(cmp_bd,StringType,true), StructField(cmp_bm,StringType,true), StructField(cmp_by,StringType,true), StructField(cmp_plz,StringType,true))\n",
       "summaryLong = [_1: string, _2: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_1: string, _2: string ... 1 more field]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ... To wide format\n",
    "val schema = summary.schema\n",
    "val summaryLong = summary.flatMap(row => {\n",
    "    val metric = row.getString(0)\n",
    "    (1 until row.size).map(i => (metric, schema(i).name, row.getString(i).toDouble))\n",
    "})\n",
    "\n",
    "summaryLong.filter($\"_2\" === \"cmp_plz\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on Implicit Type Conversion (String.toDouble)\n",
    "\n",
    "> The toDouble method is an example of one of Scala’s most powerful (and arguably dangerous) features: implicit types. In Scala, an instance of the String class is just a java.lang.String, and the java.lang.String class does not have a method named toDouble. Instead, the methods are defined in a Scala class called StringOps. Implicits work like this: if you call a method on a Scala object, and the Scala compiler does not see a definition for that method in the class definition for that object, the compiler will try to convert your object to an instance of a class that does have that method defined. In this case, the compiler will see that Java’s String class does not have a toDouble method defined but that the StringOps class does, and that the StringOps class has a method that can convert an instance of the String class into an instance of the StringOps class. The compiler silently performs the conversion of our String object into a StringOps object, and then calls the toDouble method on the new object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class org.apache.spark.sql.Dataset"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryLong.getClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dataset[T] Interface\n",
    "\n",
    "Calling `.flatMap()` on a `DataFrame` type returned the `Dataset[T]` type. `DataFrame` is just an alias for `Dataset[Row]`. In Spark 2.0 (Scala), `Dataset[T]` was added to allow handling of more data types than just `Row`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+\n",
      "|metric|  field|               value|\n",
      "+------+-------+--------------------+\n",
      "| count|cmp_plz|            573618.0|\n",
      "|  mean|cmp_plz|0.005494946113964...|\n",
      "|stddev|cmp_plz| 0.07392402321301972|\n",
      "|   min|cmp_plz|                 0.0|\n",
      "|   max|cmp_plz|                 1.0|\n",
      "+------+-------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summaryDFLong = [metric: string, field: string ... 1 more field]\n",
       "summaryDFLongShort = [metric: string, field: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[metric: string, field: string ... 1 more field]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// We can convert Dataset[T] back into a DataFrame with .toDF(columns)\n",
    "val summaryDFLong = summaryLong.toDF(\"metric\", \"field\", \"value\")\n",
    "val summaryDFLongShort = summaryDF.filter($\"field\" === \"cmp_plz\")\n",
    "summaryDFShort.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+-------------------+---+---+\n",
      "|  field|   count|                mean|             stddev|min|max|\n",
      "+-------+--------+--------------------+-------------------+---+---+\n",
      "|cmp_plz|573618.0|0.005494946113964...|0.07392402321301972|0.0|1.0|\n",
      "+-------+--------+--------------------+-------------------+---+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summaryDFWideShort = [field: string, count: double ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[field: string, count: double ... 4 more fields]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Reshaping from long back to wide\n",
    "val summaryDFWideShort = summaryDFLongShort\n",
    "    .groupBy(\"field\")\n",
    "    .pivot(\"metric\", Array(\"count\", \"mean\", \"stddev\", \"min\", \"max\"))\n",
    "    .agg(first(\"value\"))\n",
    "summaryDFWideShort.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|             cmp_plz|\n",
      "+-------+--------------------+\n",
      "|  count|              573618|\n",
      "|   mean|0.005494946113964...|\n",
      "| stddev| 0.07392402321301972|\n",
      "|    min|                   0|\n",
      "|    max|                   1|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.select(\"summary\", \"cmp_plz\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gather: (data: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "spread: (data: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---+--------------------+-------------------+---+\n",
      "|  field|   count|max|                mean|             stddev|min|\n",
      "+-------+--------+---+--------------------+-------------------+---+\n",
      "|cmp_plz|573618.0|1.0|0.005494946113964...|0.07392402321301972|0.0|\n",
      "+-------+--------+---+--------------------+-------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "def gather(data: DataFrame): DataFrame = {\n",
    "    val schema = data.schema\n",
    "    val pivoted = data.flatMap(row => {\n",
    "        val statistic = row.getString(0)\n",
    "        val rowArray = (1 until row.size)\n",
    "        rowArray.map(i => (statistic, schema(i).name, row.getString(i).toDouble))\n",
    "    })\n",
    "    pivoted.toDF(\"metric\", \"field\", \"value\")\n",
    "}\n",
    "\n",
    "def spread(data: DataFrame): DataFrame = {\n",
    "    require(data.columns.contains(\"metric\"), \"A column 'metric' must be in the DataFrame\")\n",
    "    \n",
    "    val metricColumns = Array(\"count\", \"max\", \"mean\", \"stddev\", \"min\")\n",
    "    \n",
    "    data.groupBy(\"field\")\n",
    "        .pivot(\"metric\", metricColumns)\n",
    "        .agg(first(\"value\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+\n",
      "|metric|  field|               value|\n",
      "+------+-------+--------------------+\n",
      "| count|cmp_sex|            574913.0|\n",
      "| count|cmp_plz|            573618.0|\n",
      "|  mean|cmp_sex|  0.9550923357099248|\n",
      "|  mean|cmp_plz|0.005494946113964...|\n",
      "|stddev|cmp_sex| 0.20710152240504442|\n",
      "+------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gather(summary.select(\"summary\", \"cmp_sex\", \"cmp_plz\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 476:>                                                      (0 + 0) / 200]+-------+--------+---+--------------------+-------------------+---+\n",
      "|  field|   count|max|                mean|             stddev|min|\n",
      "+-------+--------+---+--------------------+-------------------+---+\n",
      "|cmp_plz|573618.0|1.0|0.005494946113964...|0.07392402321301972|0.0|\n",
      "|cmp_sex|574913.0|1.0|  0.9550923357099248|0.20710152240504442|0.0|\n",
      "+-------+--------+---+--------------------+-------------------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summaryLongDF = [metric: string, field: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[metric: string, field: string ... 1 more field]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val summaryLongDF = summaryLong\n",
    "    .toDF(\"metric\", \"field\", \"value\")\n",
    "    .filter($\"field\" === \"cmp_sex\" || $\"field\" === \"cmp_plz\")\n",
    "spread(summaryLongDF).show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
